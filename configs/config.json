{
    "model_args": {
        "category_model": {
            "model_name_or_path": "gpt-4-1106-preview",
            "platform": "openai"
        },
        "prompt_model": {
            "model_name_or_path": "gpt-4-1106-preview",
            "platform": "openai"
        },
        "completion_model": {
            "model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2",
            "platform": "together"
        },
        "train_model": {
            "model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2",
            "platform": "huggingface"
        },
        "qualitative_eval_model": {
            "model_name_or_path": "gpt-4-1106-preview",
            "platform": "openai"
        }
    },
    "sample_args": {
        "scope": ["regional", "local"],
        "type": ["quantitative"],
        "num_feedbacks": 1,
        "prompts_per_category": 4,
        "num_prompts": 8,
        "num_negative_prompts": 8,
        "num_general_prompts": 8,
        "overwrite": false
    },
    "training_args": {
        "algo": "dpo",
        "max_prompts": 64,
        "negative_prompt_ratio": 0.8,
        "general_prompt_ratio": 0.2,
        "filter_relevant_feedback": true,
        "lora_enable": true,
        "lora_r": 128,
        "lora_alpha": 256,
        "lora_dropout": 0.05,
        "lora_bias": "none",
        "lora_exclude": ["lm_head"],
        "report_to": "wandb",
        "output_dir": "./output",
        "wandb_project": "action-fuyu",
        "num_train_epochs": 1,
        "per_device_train_batch_size": 1,
        "per_device_eval_batch_size": 2,
        "gradient_accumulation_steps": 16,
        "learning_rate": 5e-5,
        "lr_scheduler_type": "cosine",
        "save_strategy": "no",
        "evaluation_strategy": "no",
        "logging_strategy": "steps",
        "logging_steps": 1,
        "bf16": false,
        "save_safetensors": true,
        "tf32": false,
        "gradient_checkpointing": true,
        "dataloader_num_workers": 4,
        "remove_unused_columns": false, 
        "overwrite_output_dir": true,
        "dpo_beta": 0.1
    },
    "eval_args": {
        "algo": "dpo",
        "num_prompts": 64,
        "num_negative_prompts": 64
    }
}