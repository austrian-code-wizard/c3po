{
    "model_args": {
        "category_model": {
            "model_name_or_path": "gpt-3.5-turbo-1106",
            "platform": "openai"
        },
        "prompt_model": {
            "model_name_or_path": "gpt-3.5-turbo-1106",
            "platform": "openai"
        },
        "completion_model": {
            "model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2",
            "platform": "together"
        },
        "train_model": {
            "model_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2",
            "platform": "huggingface"
        }
    },
    "data_args": {
        "scope": ["global_", "regional", "local"],
        "type": ["quantitative"],
        "num_feedbacks": 1,
        "prompts_per_category": 2,
        "num_train_prompts_per_feedback": 4,
        "num_eval_prompts_per_feedback": 4,
        "num_train_prompts_general": 4,
        "num_eval_prompts_general": 4
    },
    "training_args": {
        "lora_enable": true,
        "lora_r": 16,
        "lora_alpha": 32,
        "lora_dropout": 0.05,
        "lora_bias": "none",
        "lora_exclude": ["lm_head"],
        "report_to": "wandb",
        "wandb_project": "action-fuyu",
        "output_dir": "/results/",
        "num_train_epochs": 1,
        "per_device_train_batch_size": 2,
        "per_device_eval_batch_size": 2,
        "gradient_accumulation_steps": 8,
        "learning_rate": 5e-5,
        "lr_scheduler_type": "cosine",
        "save_strategy": "steps",
        "logging_strategy": "steps",
        "evaluation_strategy": "no",
        "logging_steps": 1,
        "save_steps": 110,
        "warmup_ratio": 0.03,
        "bf16": false,
        "save_safetensors": true,
        "tf32": false,
        "gradient_checkpointing": false,
        "dataloader_num_workers": 4
    }
}